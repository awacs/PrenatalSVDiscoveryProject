# SV detection pipeline

Talkowski Lab structural variant detection pipeline. Documentation in progress.

## Dependencies

### Python 3

The pipeline requires Python 3. If you already have an established Python 3
environment, you can ignore this subsection. Otherwise, the recommended way to
create a Python 3 environment with the required packages is with
[Anaconda](https://www.continuum.io/downloads).

```
$ conda create -n $environment -c bioconda python=3.5 numpy scipy pysam snakemake
$ source activate $environment
```

### Snakemake
The pipeline is built with `snakemake`, which can be installed through `pip` or
`conda` with one of the two following commands.

```
$ pip install snakemake
$ conda install -c bioconda snakemake
```

`snakemake` is an excellent and recommended tool for building bioinformatics
pipelines, but a comprehensive understanding of the tool is not necessary to
run this pipeline. If interested in learning more, extended `snakemake`
documentation can be found on their [Read the Docs
page](https://snakemake.readthedocs.io/en/stable/). A
[tutorial](https://snakemake.bitbucket.io/snakemake-tutorial.html) and
[demonstrative slides](http://slides.com/johanneskoester/deck-1#/) are also
available. 

### SVtools
The pipeline requires the `svtools` Python package, which is currently
available only on github.

```
$ git clone git@github.com:talkowski-lab/svtools.git
$ cd svtools
$ pip install -e .
```

### Bedtools
The pipeline requires bedtools 2.26 or later. Earlier versions may throw an
error when `bedtools merge` is passed an empty file.

### pybedtools
In order to perform per-chromosome parallelization, the master branch of 
`pybedtools` is required (or at least commit `b1e0ce0`).

```
$ pip install git+git://github.com/daler/pybedtools.git@master
$ pip install git+git://github.com/daler/pybedtools.git@b1e0ce0
```

## Installation and Usage
As a `snakemake` workflow, the pipeline is intended to be cloned for each
project, e.g.

```
$ git clone git@github.com:talkowski-lab/sv-pipeline.git MySVDiscoveryProject
```

After cloning the pipeline, edit `config.yaml` to update the configuration as
necessary for the project, then link or copy raw data into the `data/` or
`ref/` directories. (More detail below or to come. For current testing
purposes, symlink the `data/` and `ref/` directories in
`/data/talkowski/Samples/SFARI/deep_sv/asc_540/sv-pipeline-devel/`). 

Optionally, run a dry run of `snakemake` to test the configuration, then run
the pipeline with `snakemake`.

```
$ vim config.yaml
$ ln -s ${raw_SV_calls} data/
$ cp ${reference_data} ref/
$ snakemake -np
$ snakemake
```

The pipeline can remove all files it has generated without affecting
configuration or data files.

```
$ snakemake clean
```

## Rolling your own preprocessing

The preprocessing module here is provided for reproducibility and as an
example implementation of SV algorithm standardization, but is not intended to
be generalizable to all use cases. 

If you would like to implement your own preprocessing before beginning
integration and filtering, the pipeline can be bootstrapped to begin with the
integration module by providing the following input files:

* `preprocessing/filtered_vcfs/{source}.{quad}.vcf.gz`  
    Tabix-indexed PE/SR VCFs per algorithm. 
    - {source} corresponds to the name of the source algorithm.
    - {quad} corresponds to an ID for a subgroup of samples (historically a
      quad in the SSC cohort; may rename this to "group" for clarity.)
    - VCF records are required to include INFO fields for SVTYPE, CHR2, END,
      STRANDS [++,+-,-+,--], SVLEN, and SOURCES.  Currently, DEL, DUP, INV, and
      BND are supported SVTYPEs; INS have not been tested. BND and INV
      breakpoints must be segregated and annotated with strand.
* `preprocessing/std_beds/cohort.{svtype}.bed.gz`  
    All per-sample depth calls, merged across algorithms.
    - Depth calls must be segregated by {svtype}, which may be DEL or DUP.
    - Each line corresponds to a single call in a single sample.
    - First six columns must be chrom, start, end, name, sample, svtype. Any
      following columns are discarded during integration.
    - In our preprocessing module, these files are generated by taking all
      algorithm's calls for a given sample and performing a bedtools merge,
      then concatenating and sorting all sample calls.

## Pipeline configuration

All variables controlling pipeline operation can be modified in `config.yaml`.
This documentation is incomplete and in progress.

* `quads` : filepath  
    Path to list of quads (or other subgroup identifier) on which to run the
pipeline. (Currently required. If only one subgroup exists, simply provide one
ID for it.)

* `samples` : filepath  
    Path to list of all samples represented in the union of quads (or other
subgroups).

* `pesr_sources` : list of strings  
    List of all PE/SR algorithms to merge.

* `svtypes` : list of strings  
    List of SV classes to consider when combining PE/SR variants. (Default:
DEL,DUP,INV,BND)

---
<center> Reliable documentation ends here </center>

---

# Draft docs

## Input data

### PE/SR calls
PE/SR calls should be placed in the `data/raw_vcfs/` directory and follow the
filename convention `{source}.{quad}.vcf.gz`, where `source` refers to the
source algorithm which generated the calls and `quad` refers to an identifier
of the batch on which the algorithm was run. In the SSC analyses, each
algorithm was run on a per-quad basis.

### Read depth calls
To be completed.

## Pipeline modules

* `preprocessing/`  
    Data standardization and outlier removal.
* `integration/`  
    Algorithm and PE/SR+RD integration.
* `rdtest/`  
    RdTest runs.

## Data, scripts, and workflows
* `data/`  
    Raw input data.
* `rules/`  
    Snakemake rules governing pipeline operation.
* `ref/`  
    Reference data and configuration files (e.g. sample lists).
* `logs/`  
    LSF logs.
* `scripts/`  
    General purpose scripts. Scripts which invoke snakemake data can be found
    in `rules/scripts/`. 
