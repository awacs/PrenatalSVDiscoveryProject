# Algorithm integration

Per-algorithm integration is the first step of the pipeline after SV calls have
been standardized. 

This workflow considers samples to belong to a variant calling group, which is
a subset of a larger processing batch. For example, samples may have been sequenced in two batches, which should be cons
one , within which 
a family which was called jointly, 

The workflow thus expects a batch key, e.g.

```
sample group batch
11194.fa 11194 pcr_plus
11006.mo 11006 pcr_minus
```



## Input files

Variant calls must be standardized to the following specifications before
integration. See the example preprocessing module for guidance.

* `pesr/input_vcfs/{source}.{group}.vcf.gz`  
    Tabix-indexed PE/SR VCFs per algorithm. 
    - {source} indicates the source algorithm.
    - {group} indicates a subgroup of samples which were called jointly.
    - VCF records are required to include INFO fields for SVTYPE, CHR2, END,
      STRANDS [++,+-,-+,--], SVLEN, and SOURCES.  Currently, DEL, DUP, INV, and
      BND are supported SVTYPEs; INS have not been tested. BND and INV
      breakpoints must be segregated and annotated with strand.  
      (NOTE: END has become a reserved attribute in pysam 0.11.2.1 and may be
      deprecated here in the future.)
* `depth/input_beds/{batch}.{svtype}.bed.gz`  
    All per-sample depth calls in a batch, merged across algorithms.
    - Depth calls must be segregated by {svtype}, which may be DEL or DUP.
    - Each line corresponds to a single call in a single sample.
    - First six columns must be chrom, start, end, name, sample, svtype. Any
      following columns are discarded during integration.
    - In our preprocessing module, these files are generated by taking all
      algorithms' calls for a given sample and performing a bedtools merge,
      then concatenating and sorting all sample calls.

## Output files

* `pesr/vcfcluster/{batch}.{source}.{chrom}.vcf`  
    Clustered PE/SR variants, per batch, per source, and per chromosome.
* `pesr/rdtest_beds/{batch}.{source}.{chrom}.bed`  
    Clustered PE/SR variants in RdTest format. 
* `depth/rdtest_beds/{batch}.depth.{chrom}.bed`  
    Clustered depth variants, per batch and per chromosome, in RdTest format.

## Configuration




## Rolling your own preprocessing

The preprocessing module here is provided for reproducibility and as an
example implementation of SV algorithm standardization, but is not intended to
be generalizable to all use cases. 

If you would like to implement your own preprocessing before beginning
integration and filtering, the pipeline can be bootstrapped to begin with the
integration module by providing the following input files:

* `preprocessing/filtered_vcfs/{source}.{quad}.vcf.gz`  
* `preprocessing/std_beds/cohort.{svtype}.bed.gz`  

## Pipeline configuration

All variables controlling pipeline operation can be modified in `config.yaml`.
This documentation is incomplete and in progress.

* `quads` : filepath  
    Path to list of quads (or other subgroup identifier) on which to run the
pipeline. (Currently required. If only one subgroup exists, simply provide one
ID for it.)

* `samples` : filepath  
    Path to list of all samples represented in the union of quads (or other
subgroups).

* `pesr_sources` : list of strings  
    List of all PE/SR algorithms to merge.

* `svtypes` : list of strings  
    List of SV classes to consider when combining PE/SR variants. (Default:
DEL,DUP,INV,BND)

* `cnv_types`: list of strings  
    List of SV classes to consider when combining depth variants. (Default:
DEL,DUP)

* `chroms`: filepath
    List of chromosomes to consider. Integration and filtering will be
parallelized by chromosome.

---
<p align="center"> Reliable documentation ends here </p>

---

# Draft docs

## Input data

### PE/SR calls
PE/SR calls should be placed in the `data/raw_vcfs/` directory and follow the
filename convention `{source}.{quad}.vcf.gz`, where `source` refers to the
source algorithm which generated the calls and `quad` refers to an identifier
of the batch on which the algorithm was run. In the SSC analyses, each
algorithm was run on a per-quad basis.

### Read depth calls
To be completed.

## Pipeline modules

* `preprocessing/`  
    Data standardization and outlier removal.
* `integration/`  
    Algorithm and PE/SR+RD integration.
* `rdtest/`  
    RdTest runs.

## Data, scripts, and workflows
* `data/`  
    Raw input data.
* `rules/`  
    Snakemake rules governing pipeline operation.
* `ref/`  
    Reference data and configuration files (e.g. sample lists).
* `logs/`  
    LSF logs.
* `scripts/`  
    General purpose scripts. Scripts which invoke snakemake data can be found
    in `rules/scripts/`. 
