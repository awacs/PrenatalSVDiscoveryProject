#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright Â© 2017 Matthew Stone <mstone5@mgh.harvard.edu>
# Distributed under terms of the MIT license.

"""

"""

import argparse
import io
from collections import deque
import numpy as np
import scipy.stats as ss
import pandas as pd
import pysam


class CNV:
    def __init__(self, chrom, start, end, name, samples, svtype):
        self.chrom = chrom
        self.start = int(start)
        self.end = int(end)
        self.name = name
        self.samples = samples.split(',')
        self.svtype = svtype.upper()

        if self.svtype not in 'DEL DUP'.split():
            msg = 'Invalid SV type: {0} [expected DEL,DUP]'.format(self.svtype)
            raise Exception(msg)

        self.background = []

    def choose_background(self, samples, n_background):
        """
        Choose background samples

        Parameters
        ----------
        samples : list of str
            All available samples
        n_background : int
            Max number of background samples
        """

        self.background = [s for s in samples if s not in self.samples]
        if len(self.background) >= n_background:
            self.background = np.random.choice(self.background, n_background,
                                               replace=False).tolist()

    def load_counts(self, countfile, window):
        """
        Generate dataframe of split counts

        Parameters
        ----------
        countfile : pysam.TabixFile
            chrom pos clip count sample
        window : int
        """

        # Check if regions overlap so duplicate lines aren't fetched
        if self.end - self.start <= window:
            regions = ['{0}:{1}-{2}'.format(self.chrom, self.start - window,
                                            self.end + window)]
        else:
            regions = []
            for pos in (self.start, self.end):
                region = '{0}:{1}-{2}'.format(self.chrom, pos - window,
                                              pos + window)
                regions.append(region)

        counts = deque()
        for region in regions:
            lines = countfile.fetch(region=region)
            lines = [l for l in lines]
            counts.append('\n'.join(lines))

        counts = io.StringIO('\n'.join(counts))

        cols = 'chrom pos clip count sample'.split()
        dtypes = dict(chrom=str, pos=int, clip=str, count=int, sample=str)
        self.split_counts = pd.read_table(counts, names=cols, dtype=dtypes)

    def process_counts(self, window):
        """
        Filter to called/background samples and assign coordinates

        Parameters
        ----------
        window : int
        """

        counts = self.split_counts

        # Restrict to called or background samples
        samples = self.samples + self.background
        counts = counts.loc[counts['sample'].isin(samples)].copy()

        # Determine whether clipped read supports start or end
        COORD_MAP = {
            'DEL': dict(right='start', left='end'),
            'DUP': dict(right='end', left='start')
        }
        counts['coord'] = counts['clip'].replace(COORD_MAP[self.svtype])

        # Filter to within window
        # (excludes spurious left/right clips at other coord)
        self.add_dists(counts)
        counts = counts.loc[counts['dist'].abs() <= window].copy()

        self.split_counts = counts

        # Fill empty samples
        self.fill_counts()

        # Label samples with background
        is_called = self.split_counts['sample'].isin(self.samples)
        if is_called.any():
            self.split_counts.loc[is_called, 'call_status'] = 'called'
        if (~is_called).any():
            self.split_counts.loc[~is_called, 'call_status'] = 'background'

    def add_dists(self, df):
        # Get distance of clip position from variant start/end
        def _coord_dist(row):
            coord = row['coord']  # 'start' or 'end'
            pos = getattr(self, coord)
            return pos - row.pos
        df['dist'] = df.apply(_coord_dist, axis=1)

    def fill_counts(self):
        """
        Fill zeros in for samples with no observed splits
        """
        counts = self.split_counts
        samples = self.samples + self.background

        sub_dfs = []
        cols = 'sample pos count'.split()
        for coord in 'start end'.split():
            df = counts.loc[counts.coord == coord, cols]

            # Consider only positions found in a called sample
            pos = df.loc[df['sample'].isin(self.samples), 'pos']
            pos = pos.unique()

            idx = pd.MultiIndex.from_product(iterables=[samples, pos])

            df = df.set_index('sample pos'.split())
            df = df.reindex(idx).fillna(0).astype(int).reset_index()
            df = df.rename(columns=dict(level_0='sample', level_1='pos'))

            df['coord'] = coord
            sub_dfs.append(df)

        self.split_counts = pd.concat(sub_dfs)

    def ttest_counts(self):
        pvals = self.split_counts.groupby('coord pos'.split())\
                                 .apply(self.calc_ttest)
        cols = {
            0: 'called_mean',
            1: 'called_std',
            2: 'bg_mean',
            3: 'bg_std',
            4: 'called_n',
            5: 'bg_n',
            6: 'log_pval'
        }

        self.pvals = pvals.rename(columns=cols).reset_index()

    def choose_best_coords(self):
        # Pick coordinates with most significant enrichment
        max_pvals = self.pvals.groupby('coord')['log_pval'].max()\
                         .reset_index()\
                         .rename(columns={'log_pval': 'max_pval'})

        pvals = pd.merge(self.pvals, max_pvals, on='coord', how='left')
        pvals = pvals.loc[pvals.log_pval == pvals.max_pval].copy()
        pvals = pvals.drop('max_pval', axis=1)

        for coord in 'start end'.split():
            if coord not in pvals.coord.values:
                try:
                    pvals = pd.concat([pvals, self.null_series(coord)])
                except:
                    import ipdb
                    ipdb.set_trace()

        # Use distance as tiebreaker
        if pvals.shape[0] > 2:
            self.add_dists(pvals)
            closest = pvals.groupby('coord')['dist'].min().reset_index()\
                           .rename(columns={'dist': 'min_dist'})
            pvals = pd.merge(pvals, closest, on='coord', how='left')
            pvals = pvals.loc[pvals.dist == pvals.min_dist].copy()
            pvals = pvals.drop('dist min_dist'.split(), axis=1)

        pvals['name'] = self.name
        self.best_pvals = pvals

    def sr_test(self, samples, countfile, n_background, window):
        self.choose_background(samples, n_background)
        self.load_counts(countfile, window)
        self.process_counts(window)

        if self.split_counts.shape[0] == 0:
            self.null_score()
        else:
            self.ttest_counts()
            self.choose_best_coords()

    def null_score(self):
        cols = ('coord pos called_mean called_std bg_mean bg_std called_n '
                'bg_n log_pval name').split()
        self.best_pvals = pd.DataFrame([
            ['end', 0, 0, 0, 0, 0, 0, 0, 0, self.name],
            ['start', 0, 0, 0, 0, 0, 0, 0, 0, self.name]],
            columns=cols)

    def null_series(self, coord):
        cols = ('coord pos called_mean called_std bg_mean bg_std called_n '
                'bg_n log_pval name').split()
        return pd.DataFrame([[coord, 0, 0, 0, 0, 0, 0, 0, 0, self.name]],
                            columns=cols)

    @staticmethod
    def calc_ttest(df):
        def _summary_stats(series):
            return series.mean(), series.std(), series.shape[0]

        called = df.loc[df.call_status == 'called', 'count']
        called_mu, called_sigma, called_n = _summary_stats(called)

        background = df.loc[df.call_status == 'background', 'count']
        bg_mu, bg_sigma, bg_n = _summary_stats(background)

        if bg_n == 0:
            p = 1
        else:
            t, _p = ss.ttest_ind_from_stats(called_mu, called_sigma, called_n,
                                            bg_mu, bg_sigma, bg_n)

            # One-sided test
            p = _p / 2 if t > 0 else 1

        return pd.Series([called_mu, called_sigma, bg_mu, bg_sigma,
                          called_n, bg_n, -np.log10(p)])


def BedParser(bedfile):
    for line in bedfile:
        yield CNV(*line.strip().split()[:6])


class SRTest():
    def __init__(self, bed, samples, countfile, window=100, n_background=160):
        """
        bed : file
            chrom start end name samples svtype
        samples : list of str
            List of all samples in cohort
        countfile : pysam.TabixFile
            chrom pos clip count sample
        window : int
            Window around CNV start/end to consider for split read enrichment
        n_background : int
            Number of background samples to choose for comparison in t-test
        """

        self.cnvs = BedParser(bed)
        self.countfile = countfile
        self.samples = sorted(samples)
        self.window = window
        self.n_background = n_background
        self.pvals = None

    def run(self):
        pvals = deque()
        for cnv in self.cnvs:
            cnv.sr_test(self.samples, self.countfile, self.n_background,
                        self.window)
            pvals.append(cnv.best_pvals)

        pvals = pd.concat(pvals)

        cols = ('name coord pos log_pval called_mean called_std bg_mean '
                'bg_std called_n bg_n').split()
        self.pvals = pvals[cols].fillna(0)

        for col in 'pos called_n bg_n'.split():
            self.pvals[col] = self.pvals[col].astype(int)


def main():
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('bed', type=argparse.FileType('r'),
                        help='Bed of CNV calls. First six columns: '
                        'chrom,start,end,name,samples,svtype')
    parser.add_argument('samples', help='File listing all sample IDs.')
    parser.add_argument('counts', help='Tabix indexed file of split counts. '
                        'Columns: chrom,pos,clip,count,sample')
    parser.add_argument('fout', help='Output table of most significant start/'
                        'end positions')
    parser.add_argument('-w', '--window', type=int, default=100,
                        help='Window around CNV start/end to consider for '
                        'split read support.')
    parser.add_argument('-b', '--background', type=int, default=160,
                        help='Number of background samples to choose for '
                        'comparison in t-test [160]')
    args = parser.parse_args()

    with open(args.samples) as slist:
        samples = [s.strip() for s in slist.readlines()]

    countfile = pysam.TabixFile(args.counts)

    srtest = SRTest(args.bed, samples, countfile, args.window, args.background)
    srtest.run()
    srtest.pvals.to_csv(args.fout, sep='\t', index=False)


if __name__ == '__main__':
    main()
